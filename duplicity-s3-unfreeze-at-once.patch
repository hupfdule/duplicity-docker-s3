diff --git a/duplicity/backends/_boto_single.py b/duplicity/backends/_boto_single.py
index c5f27ff8..ca45c194 100644
--- a/duplicity/backends/_boto_single.py
+++ b/duplicity/backends/_boto_single.py
@@ -22,6 +22,7 @@
 from __future__ import division
 from builtins import str
 import os
+import threading
 import time
 
 import duplicity.backend
@@ -339,3 +340,12 @@ class BotoBackend(duplicity.backend.Backend):
                     time.sleep(60)
                     self.resetConnection()
                 log.Info(u"File %s was successfully restored from Glacier" % remote_filename)
+
+    def pre_process_download_batch(self, remote_filenames):
+        log.Info(u"Starting batch unfreezing from Glacier")
+        # Used primarily to move all necessary files in Glacier to S3 at once
+        for remote_filename in remote_filenames:
+            remote_filename = util.fsdecode(remote_filename)
+            t= threading.Thread(target=self.pre_process_download,
+                    kwargs={'remote_filename':remote_filename, 'wait':False})
+            t.start()
diff --git a/duplicity/dup_main.py b/duplicity/dup_main.py
index cbfcf032..fe394e5f 100644
--- a/duplicity/dup_main.py
+++ b/duplicity/dup_main.py
@@ -748,6 +748,10 @@ def restore_get_patched_rop_iter(col_stats):
         u"""Get file object iterator from backup_set contain given index"""
         manifest = backup_set.get_manifest()
         volumes = manifest.get_containing_volumes(index)
+
+        if hasattr(backup_set.backend.backend, u'pre_process_download_batch'):
+            backup_set.backend.backend.pre_process_download_batch(backup_set.volume_name_dict.values())
+
         for vol_num in volumes:
             yield restore_get_enc_fileobj(backup_set.backend,
                                           backup_set.volume_name_dict[vol_num],
